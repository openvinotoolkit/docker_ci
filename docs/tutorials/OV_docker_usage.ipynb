{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOpibzYoLyJ2"
   },
   "source": [
    "# Using OpenVINO Docker containers for development and deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4ysbemILyJ_"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to demonstrate how to use OpenVINO tools and the runtime environment.\n",
    "\n",
    "This tutorial will go step-by-step to demonstrate how to download a model from OpenVINO Model Zoo, convert it to IR format and run the benchmarking applicaation. It shows also how to use sample and demo apps. Finally, it presentes to to use AI accelerators inside the containers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm1M7yufLyJ-"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial requires the following:\n",
    "- Unix* host\n",
    "- Installed and run [Docker](https://docs.docker.com/engine/install/)* engine/service on the host\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwJtjECwTT3o"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "OpenVINO Toolkit can be used in a container using one of the prebuilt docker images. It might be convenient if you don't want to install the environment on the host.\n",
    "\n",
    "We have two Docker image distributions:\n",
    "\n",
    "*   **runtime** - contains OpenVINO Runtime (Inference Engine core, nGraph) libs for each supported device (CPU, GNA, GPU, VPU)\n",
    "*   **dev** - contains runtime part plus samples and Python development tools: Model Optimizer, Post training Optimization tool (POT), Accuracy checker, Open Model Zoo tools (downloader, converter)\n",
    "\n",
    "You can search OpenVINO Docker images on Docker Hub or via cli:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO-6jBXgtPlz"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "docker search --format \"{{.Name}}: {{.Description}}\" --no-trunc --limit 100  openvino | grep ^openvino/ | grep -v DEPRECATED\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZyb7SLEtdjN"
   },
   "source": [
    "\n",
    "Output will be like this:\n",
    "```\n",
    "openvino/workbench: OpenVINO™ DL Workbench is a web GUI to simplify DL models inference and tuning on Intel® devices.\n",
    "openvino/ubuntu18_runtime: Intel® Distribution of OpenVINO™ toolkit Docker image for Ubuntu* 18.04 LTS\n",
    "openvino/ubuntu18_dev: Intel® Distribution of OpenVINO™ toolkit Docker image for Ubuntu* 18.04 LTS\n",
    "openvino/ubuntu20_runtime: Intel® Distribution of OpenVINO™ toolkit Docker image for Ubuntu* 20.04 LTS\n",
    "openvino/model_server: Intel® Distribution of OpenVINO™ Model Server Docker images\n",
    "openvino/cvat_ui: Computer Vision Annotation Tool (CVAT) frontend UI image\n",
    "openvino/cvat_server: Computer Vision Annotation Tool (CVAT) annotation backend image\n",
    "openvino/winserver2019_runtime: Intel® Distribution of OpenVINO™ toolkit Docker image for Windows Server Core base LTSC 2019\n",
    "openvino/ubuntu20_dev: Intel® Distribution of OpenVINO™ toolkit Docker image for Ubuntu* 20.04 LTS\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr2ch7tcLyKA"
   },
   "source": [
    "## Part 1. Using OpenVINO development Docker container for model conversion and execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgfntt_SLyKA"
   },
   "source": [
    "### 1.\tPull dev OpenVINO docker image and run it to download a model from the OpenVINO Model Zoo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The container can be used to run all the included tools without the need to install them on the host. Here we will use the tool `omz_downloader`. The command below can list all the models available in the OpenVINO Model Zoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sphereface\r\n",
      "aclnet\r\n",
      "aclnet-int8\r\n",
      "action-recognition-0001\r\n",
      "age-gender-recognition-retail-0013\r\n",
      "alexnet\r\n",
      "anti-spoof-mn3\r\n",
      "asl-recognition-0004\r\n",
      "background-matting-mobilenetv2\r\n",
      "bert-base-ner\r\n",
      "bert-large-uncased-whole-word-masking-squad-0001\r\n",
      "bert-large-uncased-whole-word-masking-squad-emb-0001\r\n",
      "bert-large-uncased-whole-word-masking-squad-int8-0001\r\n",
      "bert-small-uncased-whole-word-masking-squad-0001\r\n",
      "bert-small-uncased-whole-word-masking-squad-0002\r\n",
      "bert-small-uncased-whole-word-masking-squad-emb-int8-0001\r\n",
      "bert-small-uncased-whole-word-masking-squad-int8-0002\r\n",
      "brain-tumor-segmentation-0001\r\n",
      "brain-tumor-segmentation-0002\r\n",
      "caffenet\r\n",
      "cocosnet\r\n",
      "colorization-siggraph\r\n",
      "colorization-v2\r\n",
      "common-sign-language-0001\r\n",
      "common-sign-language-0002\r\n",
      "convnext-tiny\r\n",
      "ctdet_coco_dlav0_512\r\n",
      "ctpn\r\n",
      "deblurgan-v2\r\n",
      "deeplabv3\r\n",
      "densenet-121\r\n",
      "densenet-121-tf\r\n",
      "detr-resnet50\r\n",
      "dla-34\r\n",
      "driver-action-recognition-adas-0002\r\n",
      "drn-d-38\r\n",
      "efficientdet-d0-tf\r\n",
      "efficientdet-d1-tf\r\n",
      "efficientnet-b0\r\n",
      "efficientnet-b0-pytorch\r\n",
      "efficientnet-v2-b0\r\n",
      "efficientnet-v2-s\r\n",
      "emotions-recognition-retail-0003\r\n",
      "f3net\r\n",
      "face-detection-0200\r\n",
      "face-detection-0202\r\n",
      "face-detection-0204\r\n",
      "face-detection-0205\r\n",
      "face-detection-0206\r\n",
      "face-detection-adas-0001\r\n",
      "face-detection-retail-0004\r\n",
      "face-detection-retail-0005\r\n",
      "face-detection-retail-0044\r\n",
      "face-recognition-resnet100-arcface-onnx\r\n",
      "face-reidentification-retail-0095\r\n",
      "faceboxes-pytorch\r\n",
      "facenet-20180408-102900\r\n",
      "facial-landmarks-35-adas-0002\r\n",
      "facial-landmarks-98-detection-0001\r\n",
      "fast-neural-style-mosaic-onnx\r\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001\r\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "faster_rcnn_resnet50_coco\r\n",
      "fastseg-large\r\n",
      "fastseg-small\r\n",
      "fbcnn\r\n",
      "fcrn-dp-nyu-depth-v2-tf\r\n",
      "formula-recognition-medium-scan-0001\r\n",
      "formula-recognition-polynomials-handwritten-0001\r\n",
      "forward-tacotron\r\n",
      "gaze-estimation-adas-0002\r\n",
      "gmcnn-places2-tf\r\n",
      "googlenet-v1\r\n",
      "googlenet-v1-tf\r\n",
      "googlenet-v2\r\n",
      "googlenet-v2-tf\r\n",
      "googlenet-v3\r\n",
      "googlenet-v3-pytorch\r\n",
      "googlenet-v4-tf\r\n",
      "gpt-2\r\n",
      "handwritten-english-recognition-0001\r\n",
      "handwritten-japanese-recognition-0001\r\n",
      "handwritten-score-recognition-0003\r\n",
      "handwritten-simplified-chinese-recognition-0001\r\n",
      "hbonet-0.25\r\n",
      "hbonet-1.0\r\n",
      "head-pose-estimation-adas-0001\r\n",
      "higher-hrnet-w32-human-pose-estimation\r\n",
      "horizontal-text-detection-0001\r\n",
      "hrnet-v2-c1-segmentation\r\n",
      "human-pose-estimation-0001\r\n",
      "human-pose-estimation-0005\r\n",
      "human-pose-estimation-0006\r\n",
      "human-pose-estimation-0007\r\n",
      "human-pose-estimation-3d-0001\r\n",
      "hybrid-cs-model-mri\r\n",
      "i3d-rgb-tf\r\n",
      "icnet-camvid-ava-0001\r\n",
      "icnet-camvid-ava-sparse-30-0001\r\n",
      "icnet-camvid-ava-sparse-60-0001\r\n",
      "image-retrieval-0001\r\n",
      "inception-resnet-v2-tf\r\n",
      "instance-segmentation-person-0007\r\n",
      "instance-segmentation-security-0002\r\n",
      "instance-segmentation-security-0091\r\n",
      "instance-segmentation-security-0228\r\n",
      "instance-segmentation-security-1039\r\n",
      "instance-segmentation-security-1040\r\n",
      "landmarks-regression-retail-0009\r\n",
      "levit-128s\r\n",
      "license-plate-recognition-barrier-0001\r\n",
      "license-plate-recognition-barrier-0007\r\n",
      "machine-translation-nar-de-en-0002\r\n",
      "machine-translation-nar-en-de-0002\r\n",
      "machine-translation-nar-en-ru-0002\r\n",
      "machine-translation-nar-ru-en-0002\r\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "mask_rcnn_resnet50_atrous_coco\r\n",
      "midasnet\r\n",
      "mixnet-l\r\n",
      "mobilefacedet-v1-mxnet\r\n",
      "mobilenet-ssd\r\n",
      "mobilenet-v1-0.25-128\r\n",
      "mobilenet-v1-1.0-224\r\n",
      "mobilenet-v1-1.0-224-tf\r\n",
      "mobilenet-v2\r\n",
      "mobilenet-v2-1.0-224\r\n",
      "mobilenet-v2-1.4-224\r\n",
      "mobilenet-v2-pytorch\r\n",
      "mobilenet-v3-large-1.0-224-paddle\r\n",
      "mobilenet-v3-large-1.0-224-tf\r\n",
      "mobilenet-v3-small-1.0-224-paddle\r\n",
      "mobilenet-v3-small-1.0-224-tf\r\n",
      "mobilenet-yolo-v4-syg\r\n",
      "modnet-photographic-portrait-matting\r\n",
      "modnet-webcam-portrait-matting\r\n",
      "mozilla-deepspeech-0.6.1\r\n",
      "mozilla-deepspeech-0.8.2\r\n",
      "mtcnn\r\n",
      "nanodet-m-1.5x-416\r\n",
      "nanodet-plus-m-1.5x-416\r\n",
      "netvlad-tf\r\n",
      "nfnet-f0\r\n",
      "noise-suppression-denseunet-ll-0001\r\n",
      "noise-suppression-poconetlike-0001\r\n",
      "ocrnet-hrnet-w48-paddle\r\n",
      "octave-resnet-26-0.25\r\n",
      "open-closed-eye-0001\r\n",
      "pedestrian-and-vehicle-detector-adas-0001\r\n",
      "pedestrian-detection-adas-0002\r\n",
      "pelee-coco\r\n",
      "person-attributes-recognition-crossroad-0230\r\n",
      "person-attributes-recognition-crossroad-0234\r\n",
      "person-attributes-recognition-crossroad-0238\r\n",
      "person-detection-0106\r\n",
      "person-detection-0200\r\n",
      "person-detection-0201\r\n",
      "person-detection-0202\r\n",
      "person-detection-0203\r\n",
      "person-detection-0301\r\n",
      "person-detection-0302\r\n",
      "person-detection-0303\r\n",
      "person-detection-action-recognition-0005\r\n",
      "person-detection-action-recognition-0006\r\n",
      "person-detection-action-recognition-teacher-0002\r\n",
      "person-detection-asl-0001\r\n",
      "person-detection-raisinghand-recognition-0001\r\n",
      "person-detection-retail-0002\r\n",
      "person-detection-retail-0013\r\n",
      "person-reidentification-retail-0277\r\n",
      "person-reidentification-retail-0286\r\n",
      "person-reidentification-retail-0287\r\n",
      "person-reidentification-retail-0288\r\n",
      "person-vehicle-bike-detection-2000\r\n",
      "person-vehicle-bike-detection-2001\r\n",
      "person-vehicle-bike-detection-2002\r\n",
      "person-vehicle-bike-detection-2003\r\n",
      "person-vehicle-bike-detection-2004\r\n",
      "person-vehicle-bike-detection-crossroad-0078\r\n",
      "person-vehicle-bike-detection-crossroad-1016\r\n",
      "person-vehicle-bike-detection-crossroad-yolov3-1020\r\n",
      "product-detection-0001\r\n",
      "pspnet-pytorch\r\n",
      "quartznet-15x5-en\r\n",
      "regnetx-3.2gf\r\n",
      "repvgg-a0\r\n",
      "repvgg-b1\r\n",
      "repvgg-b3\r\n",
      "resnest-50-pytorch\r\n",
      "resnet-18-pytorch\r\n",
      "resnet-34-pytorch\r\n",
      "resnet-50-pytorch\r\n",
      "resnet-50-tf\r\n",
      "resnet18-xnor-binary-onnx-0001\r\n",
      "resnet50-binary-0001\r\n",
      "retinaface-resnet50-pytorch\r\n",
      "retinanet-tf\r\n",
      "rexnet-v1-x1.0\r\n",
      "rfcn-resnet101-coco-tf\r\n",
      "road-segmentation-adas-0001\r\n",
      "robust-video-matting-mobilenetv3\r\n",
      "se-inception\r\n",
      "se-resnet-50\r\n",
      "se-resnext-50\r\n",
      "semantic-segmentation-adas-0001\r\n",
      "shufflenet-v2-x0.5\r\n",
      "shufflenet-v2-x1.0\r\n",
      "single-human-pose-estimation-0001\r\n",
      "single-image-super-resolution-1032\r\n",
      "single-image-super-resolution-1033\r\n",
      "smartlab-object-detection-0001\r\n",
      "smartlab-object-detection-0002\r\n",
      "smartlab-object-detection-0003\r\n",
      "smartlab-object-detection-0004\r\n",
      "smartlab-sequence-modelling-0001\r\n",
      "squeezenet1.0\r\n",
      "squeezenet1.1\r\n",
      "ssd-resnet34-1200-onnx\r\n",
      "ssd300\r\n",
      "ssd512\r\n",
      "ssd_mobilenet_v1_coco\r\n",
      "ssd_mobilenet_v1_fpn_coco\r\n",
      "ssdlite_mobilenet_v2\r\n",
      "swin-tiny-patch4-window7-224\r\n",
      "t2t-vit-14\r\n",
      "text-detection-0003\r\n",
      "text-detection-0004\r\n",
      "text-image-super-resolution-0001\r\n",
      "text-recognition-0012\r\n",
      "text-recognition-0014\r\n",
      "text-recognition-0015\r\n",
      "text-recognition-0016\r\n",
      "text-recognition-resnet-fc\r\n",
      "text-spotting-0005\r\n",
      "text-to-speech-en-0001\r\n",
      "text-to-speech-en-multi-0001\r\n",
      "time-series-forecasting-electricity-0001\r\n",
      "ultra-lightweight-face-detection-rfb-320\r\n",
      "ultra-lightweight-face-detection-slim-320\r\n",
      "unet-camvid-onnx-0001\r\n",
      "vehicle-attributes-recognition-barrier-0039\r\n",
      "vehicle-attributes-recognition-barrier-0042\r\n",
      "vehicle-detection-0200\r\n",
      "vehicle-detection-0201\r\n",
      "vehicle-detection-0202\r\n",
      "vehicle-detection-adas-0002\r\n",
      "vehicle-license-plate-detection-barrier-0106\r\n",
      "vehicle-license-plate-detection-barrier-0123\r\n",
      "vehicle-reid-0001\r\n",
      "vgg16\r\n",
      "vgg19\r\n",
      "vitstr-small-patch16-224\r\n",
      "wav2vec2-base\r\n",
      "wavernn\r\n",
      "weld-porosity-detection-0001\r\n",
      "yolact-resnet50-fpn-pytorch\r\n",
      "yolo-v1-tiny-tf\r\n",
      "yolo-v2-ava-0001\r\n",
      "yolo-v2-ava-sparse-35-0001\r\n",
      "yolo-v2-ava-sparse-70-0001\r\n",
      "yolo-v2-tf\r\n",
      "yolo-v2-tiny-ava-0001\r\n",
      "yolo-v2-tiny-ava-sparse-30-0001\r\n",
      "yolo-v2-tiny-ava-sparse-60-0001\r\n",
      "yolo-v2-tiny-tf\r\n",
      "yolo-v2-tiny-vehicle-detection-0001\r\n",
      "yolo-v3-onnx\r\n",
      "yolo-v3-tf\r\n",
      "yolo-v3-tiny-onnx\r\n",
      "yolo-v3-tiny-tf\r\n",
      "yolo-v4-tf\r\n",
      "yolo-v4-tiny-tf\r\n",
      "yolof\r\n",
      "yolox-tiny\r\n"
     ]
    }
   ],
   "source": [
    "!docker run openvino/ubuntu20_dev:2022.2.0 omz_downloader --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUVuUHCzrDzM"
   },
   "source": [
    "`docker run` command below will execute `omz_downloader` script to download `mozilla-deepspeech-0.6.1` model to `model` folder on the host.\n",
    "Note that the first time it will download the image which is several GB in size.",
    "\n",
    "* `-u $(id -u)` this parameter start to container in the current user security context. It ensures the container will have permissions in the mounted folder to save the results and keep the permissions to those saved content.\n",
    "* `--rm` option needs to remove container after execution.\n",
    "* `-v $(pwd)/model:/tmp/model` option needs to mount host folder model from the working directory to the container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIDjqDumLyKA",
    "outputId": "e5c054bd-4090-4cb3-aec9-0b34f02ea717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading mozilla-deepspeech-0.6.1 ||################\n",
      "\n",
      "========== Downloading /tmp/model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models.tar.gz\n",
      "... 0%, 1024 KB, 5030 KB/s, 0 seconds passed\n",
      "... 0%, 2048 KB, 7038 KB/s, 0 seconds passed\n",
      "... 0%, 3072 KB, 8140 KB/s, 0 seconds passed\n",
      "... 0%, 4096 KB, 9380 KB/s, 0 seconds passed\n",
      "... 0%, 5120 KB, 10131 KB/s, 0 seconds passed\n",
      "... 0%, 6144 KB, 10923 KB/s, 0 seconds passed\n",
      "... 0%, 7168 KB, 11694 KB/s, 0 seconds passed\n",
      "... 0%, 8192 KB, 12391 KB/s, 0 seconds passed\n",
      "... 0%, 9216 KB, 12886 KB/s, 0 seconds passed\n",
      "... 0%, 10240 KB, 13430 KB/s, 0 seconds passed\n",
      "... 0%, 11264 KB, 13928 KB/s, 0 seconds passed\n",
      "... 1%, 12288 KB, 14462 KB/s, 0 seconds passed\n",
      "... 1%, 13312 KB, 14911 KB/s, 0 seconds passed\n",
      "... 1%, 14336 KB, 15367 KB/s, 0 seconds passed\n",
      "... 1%, 15360 KB, 15820 KB/s, 0 seconds passed\n",
      "... 1%, 16384 KB, 16252 KB/s, 1 seconds passed\n",
      "... 1%, 17408 KB, 16634 KB/s, 1 seconds passed\n",
      "... 1%, 18432 KB, 17102 KB/s, 1 seconds passed\n",
      "... 1%, 19456 KB, 17436 KB/s, 1 seconds passed\n",
      "... 1%, 20480 KB, 17688 KB/s, 1 seconds passed\n",
      "... 1%, 21504 KB, 18015 KB/s, 1 seconds passed\n",
      "... 1%, 22528 KB, 18400 KB/s, 1 seconds passed\n",
      "... 1%, 23552 KB, 18727 KB/s, 1 seconds passed\n",
      "... 2%, 24576 KB, 18976 KB/s, 1 seconds passed\n",
      "... 2%, 25600 KB, 19211 KB/s, 1 seconds passed\n",
      "... 2%, 26624 KB, 19555 KB/s, 1 seconds passed\n",
      "... 2%, 27648 KB, 20011 KB/s, 1 seconds passed\n",
      "... 2%, 28672 KB, 20157 KB/s, 1 seconds passed\n",
      "... 2%, 29696 KB, 20471 KB/s, 1 seconds passed\n",
      "... 2%, 30720 KB, 20795 KB/s, 1 seconds passed\n",
      "... 2%, 31744 KB, 21063 KB/s, 1 seconds passed\n",
      "... 2%, 32768 KB, 21463 KB/s, 1 seconds passed\n",
      "... 2%, 33792 KB, 21784 KB/s, 1 seconds passed\n",
      "... \n",
      "... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 99%, 1188864 KB, 35832 KB/s, 33 seconds passed\n",
      "... 99%, 1189888 KB, 35855 KB/s, 33 seconds passed\n",
      "... 99%, 1190912 KB, 35841 KB/s, 33 seconds passed\n",
      "... 99%, 1191936 KB, 35861 KB/s, 33 seconds passed\n",
      "... 99%, 1192960 KB, 35766 KB/s, 33 seconds passed\n",
      "... 99%, 1193984 KB, 35757 KB/s, 33 seconds passed\n",
      "... 99%, 1195008 KB, 35777 KB/s, 33 seconds passed\n",
      "... 99%, 1196032 KB, 35764 KB/s, 33 seconds passed\n",
      "... 99%, 1197056 KB, 35784 KB/s, 33 seconds passed\n",
      "... 99%, 1198080 KB, 35768 KB/s, 33 seconds passed\n",
      "... 99%, 1199104 KB, 35725 KB/s, 33 seconds passed\n",
      "... 99%, 1200128 KB, 35745 KB/s, 33 seconds passed\n",
      "... 100%, 1200215 KB, 35741 KB/s, 33 seconds passed\n",
      "\n",
      "========== Unpacking /tmp/model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models.tar.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir model\n",
    "!docker run -u $(id -u) --rm -v $(pwd)/model:/tmp/model openvino/ubuntu20_dev:2022.2.0 omz_downloader --name mozilla-deepspeech-0.6.1 -o /tmp/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\r\n",
      "total 4\r\n",
      "drwxr-xr-x 3 dtrawins root 4096 Sep 23 13:09 public\r\n",
      "\r\n",
      "model/public:\r\n",
      "total 4\r\n",
      "drwxr-xr-x 3 dtrawins root 4096 Sep 23 13:10 mozilla-deepspeech-0.6.1\r\n",
      "\r\n",
      "model/public/mozilla-deepspeech-0.6.1:\r\n",
      "total 4\r\n",
      "drwxr-xr-x 2 dtrawins root 4096 Jan 10  2020 deepspeech-0.6.1-models\r\n",
      "\r\n",
      "model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models:\r\n",
      "total 1350668\r\n",
      "-rw-r--r-- 1 dtrawins root 945699324 Dec  3  2019 lm.binary\r\n",
      "-rw-r--r-- 1 dtrawins root 188914896 Dec  3  2019 output_graph.pb\r\n",
      "-rw-r--r-- 1 dtrawins root 188915850 Dec  3  2019 output_graph.pbmm\r\n",
      "-rw-r--r-- 1 dtrawins root  47331120 Jan 10  2020 output_graph.tflite\r\n",
      "-rw-r--r-- 1 dtrawins root  12200736 Dec  3  2019 trie\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdQgt9XNLyKB"
   },
   "source": [
    "### 2. Convert our model to the Intermediate Representation (IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrstRFrTRbHM"
   },
   "source": [
    "OpenVINO™ toolkit introduces its own format of graph representation and its own operation set. A graph is represented with two files: an XML file and a binary file. This representation is commonly referred to as the Intermediate Representation or IR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2426698SDWD"
   },
   "source": [
    "`docker run` command will run a container with `omz_converter` to convert the model stored in `/tmp/model` folder and save it to `/tmp/model/converted` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPEK2tAMLyKC",
    "outputId": "7376684f-ef31-4a9c-c4d5-767f4e623921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Converting mozilla-deepspeech-0.6.1 to IR (FP16)\n",
      "Conversion command: /usr/bin/python3.8 -- /usr/local/bin/mo --framework=tf --data_type=FP16 --output_dir=/tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP16 --model_name=mozilla-deepspeech-0.6.1 --input=input_node,previous_state_h,previous_state_c --input_model=/tmp/model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models/output_graph.pb '--freeze_placeholder_with_value=input_lengths->[16]' --output=logits,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd_1 --disable_nhwc_to_nchw '--layout=input_node(NSTC),previous_state_h(NC),previous_state_c(NC)' '--input_shape=[1, 16, 19, 26],[1, 2048],[1, 2048]'\n",
      "\n",
      "[ WARNING ]  Use of deprecated cli option --disable_nhwc_to_nchw detected. Option use in the following releases will be fatal. \n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/tmp/model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models/output_graph.pb\n",
      "\t- Path for generated IR: \t/tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP16\n",
      "\t- IR output name: \tmozilla-deepspeech-0.6.1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tinput_node,previous_state_h,previous_state_c\n",
      "\t- Output layers: \tlogits,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd_1\n",
      "\t- Input shapes: \t[1, 16, 19, 26],[1, 2048],[1, 2048]\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tinput_node(NSTC),previous_state_h(NC),previous_state_c(NC)\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "OpenVINO runtime found in: \t/opt/intel/openvino/python/python3.8/openvino\n",
      "OpenVINO runtime version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "Model Optimizer version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP16/mozilla-deepspeech-0.6.1.xml\n",
      "[ SUCCESS ] BIN file: /tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP16/mozilla-deepspeech-0.6.1.bin\n",
      "[ SUCCESS ] Total execution time: 8.15 seconds. \n",
      "[ SUCCESS ] Memory consumed: 1309 MB. \n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n",
      "\n",
      "========== Converting mozilla-deepspeech-0.6.1 to IR (FP32)\n",
      "Conversion command: /usr/bin/python3.8 -- /usr/local/bin/mo --framework=tf --data_type=FP32 --output_dir=/tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP32 --model_name=mozilla-deepspeech-0.6.1 --input=input_node,previous_state_h,previous_state_c --input_model=/tmp/model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models/output_graph.pb '--freeze_placeholder_with_value=input_lengths->[16]' --output=logits,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd_1 --disable_nhwc_to_nchw '--layout=input_node(NSTC),previous_state_h(NC),previous_state_c(NC)' '--input_shape=[1, 16, 19, 26],[1, 2048],[1, 2048]' '--layout=input_node(NSTC),previous_state_h(NC),previous_state_c(NC)' '--input_shape=[1, 16, 19, 26],[1, 2048],[1, 2048]'\n",
      "\n",
      "[ WARNING ]  Use of deprecated cli option --disable_nhwc_to_nchw detected. Option use in the following releases will be fatal. \n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/tmp/model/public/mozilla-deepspeech-0.6.1/deepspeech-0.6.1-models/output_graph.pb\n",
      "\t- Path for generated IR: \t/tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP32\n",
      "\t- IR output name: \tmozilla-deepspeech-0.6.1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tinput_node,previous_state_h,previous_state_c\n",
      "\t- Output layers: \tlogits,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd,cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd_1\n",
      "\t- Input shapes: \t[1, 16, 19, 26],[1, 2048],[1, 2048]\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tinput_node(NSTC),previous_state_h(NC),previous_state_c(NC)\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "OpenVINO runtime found in: \t/opt/intel/openvino/python/python3.8/openvino\n",
      "OpenVINO runtime version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "Model Optimizer version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP32/mozilla-deepspeech-0.6.1.xml\n",
      "[ SUCCESS ] BIN file: /tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP32/mozilla-deepspeech-0.6.1.bin\n",
      "[ SUCCESS ] Total execution time: 7.44 seconds. \n",
      "[ SUCCESS ] Memory consumed: 1309 MB. \n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker run -u $(id -u) --rm -v $(pwd)/model:/tmp/model openvino/ubuntu20_dev:2022.2.0 omz_converter --name mozilla-deepspeech-0.6.1 -d /tmp/model -o /tmp/model/converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/converted:\r\n",
      "total 4\r\n",
      "drwxr-xr-x 3 dtrawins root 4096 Sep 23 13:43 public\r\n",
      "\r\n",
      "model/converted/public:\r\n",
      "total 4\r\n",
      "drwxr-xr-x 4 dtrawins root 4096 Sep 23 13:43 mozilla-deepspeech-0.6.1\r\n",
      "\r\n",
      "model/converted/public/mozilla-deepspeech-0.6.1:\r\n",
      "total 8\r\n",
      "drwxr-xr-x 2 dtrawins root 4096 Sep 23 13:43 FP16\r\n",
      "drwxr-xr-x 2 dtrawins root 4096 Sep 23 13:43 FP32\r\n",
      "\r\n",
      "model/converted/public/mozilla-deepspeech-0.6.1/FP16:\r\n",
      "total 92308\r\n",
      "-rw-r--r-- 1 dtrawins root 94449844 Sep 23 13:43 mozilla-deepspeech-0.6.1.bin\r\n",
      "-rw-r--r-- 1 dtrawins root    18764 Sep 23 13:43 mozilla-deepspeech-0.6.1.mapping\r\n",
      "-rw-r--r-- 1 dtrawins root    47763 Sep 23 13:43 mozilla-deepspeech-0.6.1.xml\r\n",
      "\r\n",
      "model/converted/public/mozilla-deepspeech-0.6.1/FP32:\r\n",
      "total 184540\r\n",
      "-rw-r--r-- 1 dtrawins root 188899568 Sep 23 13:43 mozilla-deepspeech-0.6.1.bin\r\n",
      "-rw-r--r-- 1 dtrawins root     18764 Sep 23 13:43 mozilla-deepspeech-0.6.1.mapping\r\n",
      "-rw-r--r-- 1 dtrawins root     39055 Sep 23 13:43 mozilla-deepspeech-0.6.1.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lR model/converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSPLNp9uLyKC"
   },
   "source": [
    "### 3. Run benchmark app on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ttb5sagS-k8"
   },
   "source": [
    "Benchmark app can be used to assess the performance of the models executed by OpenVINO runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFK9TYO8LyKC",
    "outputId": "29897fba-2dcc-4208-ee12-1ae7255b023e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Step 1/11] Parsing and validating input arguments',\n",
       " '[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. ',\n",
       " '[Step 2/11] Loading OpenVINO',\n",
       " '[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.',\n",
       " '[ INFO ] OpenVINO:',\n",
       " '         API version............. 2022.2.0-7713-af16ea1d79a-releases/2022/2',\n",
       " '[ INFO ] Device info',\n",
       " '         CPU',\n",
       " '         openvino_intel_cpu_plugin version 2022.2',\n",
       " '         Build................... 2022.2.0-7713-af16ea1d79a-releases/2022/2',\n",
       " '',\n",
       " '[Step 3/11] Setting device configuration',\n",
       " '[ WARNING ] -nstreams default value is determined automatically for CPU device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.',\n",
       " '[Step 4/11] Reading network files',\n",
       " '[ INFO ] Read model took 113.34 ms',\n",
       " '[Step 5/11] Resizing network to match image sizes and given batch',\n",
       " '[ INFO ] Network batch size: 1',\n",
       " '[Step 6/11] Configuring input of the model',\n",
       " \"[ INFO ] Model input 'input_node' precision f32, dimensions ([N,S,T,C]): 1 16 19 26\",\n",
       " \"[ INFO ] Model input 'previous_state_h' precision f32, dimensions ([N,C]): 1 2048\",\n",
       " \"[ INFO ] Model input 'previous_state_c' precision f32, dimensions ([N,C]): 1 2048\",\n",
       " \"[ INFO ] Model output 'logits' precision f32, dimensions ([...]): 16 1 29\",\n",
       " \"[ INFO ] Model output 'cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd' precision f32, dimensions ([...]): 1 2048\",\n",
       " \"[ INFO ] Model output 'cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/GatherNd_1' precision f32, dimensions ([...]): 1 2048\",\n",
       " '[Step 7/11] Loading the model to the device',\n",
       " '[ INFO ] Compile model took 1627.71 ms',\n",
       " '[Step 8/11] Querying optimal runtime parameters',\n",
       " '[ INFO ] DEVICE: CPU',\n",
       " \"[ INFO ]   AVAILABLE_DEVICES  , ['']\",\n",
       " '[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)',\n",
       " '[ INFO ]   RANGE_FOR_STREAMS  , (1, 96)',\n",
       " '[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz',\n",
       " \"[ INFO ]   OPTIMIZATION_CAPABILITIES  , ['WINOGRAD', 'FP32', 'FP16', 'INT8', 'BIN', 'EXPORT_IMPORT']\",\n",
       " '[ INFO ]   CACHE_DIR  , ',\n",
       " '[ INFO ]   NUM_STREAMS  , 12',\n",
       " '[ INFO ]   AFFINITY  , Affinity.CORE',\n",
       " '[ INFO ]   INFERENCE_NUM_THREADS  , 0',\n",
       " '[ INFO ]   PERF_COUNT  , False',\n",
       " \"[ INFO ]   INFERENCE_PRECISION_HINT  , <Type: 'float32'>\",\n",
       " '[ INFO ]   PERFORMANCE_HINT  , PerformanceMode.THROUGHPUT',\n",
       " '[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0',\n",
       " '[Step 9/11] Creating infer requests and preparing input data',\n",
       " '[ INFO ] Create 12 infer requests took 1.65 ms',\n",
       " \"[ WARNING ] No input files were given for input 'input_node'!. This input will be filled with random values!\",\n",
       " \"[ WARNING ] No input files were given for input 'previous_state_h'!. This input will be filled with random values!\",\n",
       " \"[ WARNING ] No input files were given for input 'previous_state_c'!. This input will be filled with random values!\",\n",
       " \"[ INFO ] Fill input 'input_node' with random values \",\n",
       " \"[ INFO ] Fill input 'previous_state_h' with random values \",\n",
       " \"[ INFO ] Fill input 'previous_state_c' with random values \",\n",
       " '[Step 10/11] Measuring performance (Start inference asynchronously, 12 inference requests using 12 streams for CPU, inference only: True, limits: 60000 ms duration)',\n",
       " '[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).',\n",
       " '[ INFO ] First inference took 86.13 ms',\n",
       " '[Step 11/11] Dumping statistics report',\n",
       " 'Count:          12396 iterations',\n",
       " 'Duration:       60078.33 ms',\n",
       " 'Latency:',\n",
       " '    Median:     52.24 ms',\n",
       " '    AVG:        57.60 ms',\n",
       " '    MIN:        40.23 ms',\n",
       " '    MAX:        115.42 ms',\n",
       " 'Throughput: 206.33 FPS']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!docker run -u $(id -u) --rm -v $(pwd)/model:/tmp/model openvino/ubuntu20_dev:2022.2.0 benchmark_app -m /tmp/model/converted/public/mozilla-deepspeech-0.6.1/FP32/mozilla-deepspeech-0.6.1.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run a demo from an OpenVINO Model Zoo\n",
    "\n",
    "Below will be ilustrated how to run the demos from the [OpenVINO Model Zoo repository](https://github.com/openvinotoolkit/open_model_zoo)\n",
    "\n",
    "Start interactive session in the container with a command:\n",
    "\n",
    "`docker run -it openvino/ubuntu20_dev:2022.2.0` \n",
    "\n",
    "and execute the following commmands:\n",
    "```\n",
    "git clone --depth=1 --recurse-submodules --shallow-submodules https://github.com/openvinotoolkit/open_model_zoo.git\n",
    "cd open_model_zoo/demos/classification_demo/python\n",
    "curl https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/3/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.xml -o resnet50-binary-0001.xml\n",
    "curl https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/3/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.bin -o resnet50-binary-0001.bin\n",
    "curl https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/zebra.jpeg -o zebra.jpeg\n",
    "python3 classification_demo.py -m resnet50-binary-0001.xml -i images/zebra.jpeg --labels ../../../data/dataset_classes/imagenet_2012.txt --no_show -nstreams 1 -r\n",
    "```\n",
    "\n",
    "The output will be like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'open_model_zoo'...\n",
      "Submodule 'demos/thirdparty/gflags' (https://github.com/gflags/gflags.git) registered for path 'demos/thirdparty/gflags'\n",
      "Cloning into '/opt/intel/openvino_2022.2.0.7713/open_model_zoo/demos/thirdparty/gflags'...\n",
      "From https://github.com/gflags/gflags\n",
      " * branch            e171aa2d15ed9eb17054558e0b3a6a413bb01067 -> FETCH_HEAD\n",
      "Submodule path 'demos/thirdparty/gflags': checked out 'e171aa2d15ed9eb17054558e0b3a6a413bb01067'\n",
      "Submodule 'doc' (https://github.com/gflags/gflags.git) registered for path 'demos/thirdparty/gflags/doc'\n",
      "Cloning into '/opt/intel/openvino_2022.2.0.7713/open_model_zoo/demos/thirdparty/gflags/doc'...\n",
      "From https://github.com/gflags/gflags\n",
      " * branch            8411df715cf522606e3b1aca386ddfc0b63d34b4 -> FETCH_HEAD\n",
      "Submodule path 'demos/thirdparty/gflags/doc': checked out '8411df715cf522606e3b1aca386ddfc0b63d34b4'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  543k  100  543k    0     0  1019k      0 --:--:-- --:--:-- --:--:-- 1019k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 21.2M  100 21.2M    0     0  13.7M      0  0:00:01  0:00:01 --:--:-- 13.6M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 39457  100 39457    0     0   101k      0 --:--:-- --:--:-- --:--:--  101k\n",
      "[ INFO ] OpenVINO Runtime\n",
      "[ INFO ] \tbuild: 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ INFO ] Reading model resnet50-binary-0001.xml\n",
      "[ INFO ] \tInput layer: 0, shape: [1, 3, 224, 224], precision: f32, layout: NCHW\n",
      "[ INFO ] \tOutput layer: 1463, shape: [1, 1000], precision: f32, layout: \n",
      "[ INFO ] The model resnet50-binary-0001.xml is loaded to CPU\n",
      "[ INFO ] \tDevice: CPU\n",
      "[ INFO ] \t\tNumber of streams: 1\n",
      "[ INFO ] \t\tNumber of threads: AUTO\n",
      "[ INFO ] \tNumber of model infer requests: 2\n",
      "[ DEBUG ]  ------------------- Frame # 0 ------------------ \n",
      "[ DEBUG ]  Class ID |      Label     | Confidence \n",
      "[ DEBUG ]    340    |      zebra     |  0.998992  \n",
      "[ DEBUG ]    390    |       eel      |  0.000294  \n",
      "[ DEBUG ]    83     | prairie chicken|  0.000249  \n",
      "[ DEBUG ]    353    |     gazelle    |  0.000246  \n",
      "[ DEBUG ]    361    |      skunk     |  0.000219  \n",
      "[ INFO ] Metrics report:\n",
      "[ INFO ] \tLatency: 16.3 ms\n",
      "[ INFO ] \tFPS: 61.5\n",
      "[ INFO ] \tDecoding:\t1.1 ms\n",
      "[ INFO ] \tPreprocessing:\t0.1 ms\n",
      "[ INFO ] \tInference:\t11.3 ms\n",
      "[ INFO ] \tPostprocessing:\t0.5 ms\n",
      "[ INFO ] \tRendering:\t3.7 ms\n"
     ]
    }
   ],
   "source": [
    "!docker run openvino/ubuntu20_dev:2022.2.0 bash -c \"git clone --depth=1 --recurse-submodules --shallow-submodules https://github.com/openvinotoolkit/open_model_zoo.git && \\\n",
    "   cd open_model_zoo/demos/classification_demo/python && \\\n",
    "   curl https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/3/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.xml -o resnet50-binary-0001.xml && \\\n",
    "   curl https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/3/resnet50-binary-0001/FP32-INT1/resnet50-binary-0001.bin -o resnet50-binary-0001.bin && \\\n",
    "   curl https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/zebra.jpeg -o zebra.jpeg && \\\n",
    "   python3 classification_demo.py -m resnet50-binary-0001.xml -i zebra.jpeg --labels ../../../data/dataset_classes/imagenet_2012.txt --no_show -nstreams 1 -r \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGd7SHHmLyKD"
   },
   "source": [
    "## Part 2. Set up OpenVINO runtime container to run OpenVINO samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjkt7OcQLyKD"
   },
   "source": [
    "The runtime image incudes a smaller number of tools and components. Most importantly it is missing openvino python development package [openvino-dev](https://pypi.org/project/openvino-dev/).\n",
    "\n",
    "Below is an example how you could use the runtime image to build and execute the sample apps.\n",
    "\n",
    "Start the container interactively:\n",
    "\n",
    "`docker run -it openvino/ubuntu20_runtime:2022.2.0`\n",
    "\n",
    "and execute the following commands:\n",
    "```\n",
    "./samples/cpp/build_samples.sh\n",
    "/home/openvino/inference_engine_cpp_samples_build/intel64/Release/hello_query_device\n",
    "```\n",
    "\n",
    "Below is the output result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d4T9ZbHmLyKD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting environment variables for building samples...\n",
      "[setupvars.sh] OpenVINO environment initialized\n",
      "-- The C compiler identification is GNU 9.4.0\n",
      "-- The CXX compiler identification is GNU 9.4.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Looking for C++ include unistd.h\n",
      "-- Looking for C++ include unistd.h - found\n",
      "-- Looking for C++ include stdint.h\n",
      "-- Looking for C++ include stdint.h - found\n",
      "-- Looking for C++ include sys/types.h\n",
      "-- Looking for C++ include sys/types.h - found\n",
      "-- Looking for C++ include fnmatch.h\n",
      "-- Looking for C++ include fnmatch.h - found\n",
      "-- Looking for strtoll\n",
      "-- Looking for strtoll - found\n",
      "-- Using the single-header code from /opt/intel/openvino_2022.2.0.7713/samples/cpp/thirdparty/nlohmann_json/single_include/\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "-- Looking for pthread_create in pthreads\n",
      "-- Looking for pthread_create in pthreads - not found\n",
      "-- Looking for pthread_create in pthread\n",
      "-- Looking for pthread_create in pthread - found\n",
      "-- Found Threads: TRUE  \n",
      "CMake Warning at common/format_reader/CMakeLists.txt:21 (message):\n",
      "  OpenCV is disabled or not found, format_reader will be built without OpenCV\n",
      "  support\n",
      "\n",
      "\n",
      "-- Looking for CL_VERSION_2_2\n",
      "-- Looking for CL_VERSION_2_2 - not found\n",
      "-- Looking for CL_VERSION_2_1\n",
      "-- Looking for CL_VERSION_2_1 - not found\n",
      "-- Looking for CL_VERSION_2_0\n",
      "-- Looking for CL_VERSION_2_0 - not found\n",
      "-- Looking for CL_VERSION_1_2\n",
      "-- Looking for CL_VERSION_1_2 - not found\n",
      "-- Looking for CL_VERSION_1_1\n",
      "-- Looking for CL_VERSION_1_1 - not found\n",
      "-- Looking for CL_VERSION_1_0\n",
      "-- Looking for CL_VERSION_1_0 - not found\n",
      "-- Could NOT find OpenCL (missing: OpenCL_LIBRARY OpenCL_INCLUDE_DIR) \n",
      "CMake Warning at benchmark_app/CMakeLists.txt:65 (message):\n",
      "  OpenCV is disabled or not found, benchmark_app will be built without OpenCV\n",
      "  support.  Set OpenCV_DIR\n",
      "\n",
      "\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/openvino/inference_engine_cpp_samples_build\n",
      "Scanning dependencies of target format_reader\n",
      "Scanning dependencies of target gflags_nothreads_static\n",
      "Scanning dependencies of target zlib\n",
      "[  1%] Building CXX object common/format_reader/CMakeFiles/format_reader.dir/format_reader.cpp.o\n",
      "[  3%] Building CXX object common/format_reader/CMakeFiles/format_reader.dir/MnistUbyte.cpp.o\n",
      "[  5%] Building CXX object thirdparty/gflags/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags.cc.o\n",
      "[  7%] Building CXX object common/format_reader/CMakeFiles/format_reader.dir/bmp.cpp.o\n",
      "[  9%] Building CXX object common/format_reader/CMakeFiles/format_reader.dir/opencv_wrapper.cpp.o\n",
      "[ 13%] Building CXX object common/format_reader/CMakeFiles/format_reader.dir/yuv_nv12.cpp.o\n",
      "[ 13%] Building CXX object thirdparty/gflags/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags_reporting.cc.o\n",
      "[ 15%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/adler32.c.o\n",
      "[ 16%] Building CXX object thirdparty/gflags/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags_completions.cc.o\n",
      "[ 18%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/compress.c.o\n",
      "[ 20%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/crc32.c.o\n",
      "[ 22%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/deflate.c.o\n",
      "[ 24%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/gzlib.c.o\n",
      "[ 26%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/gzclose.c.o\n",
      "[ 28%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/gzread.c.o\n",
      "[ 30%] Linking CXX shared library ../../intel64/Release/lib/libformat_reader.so\n",
      "[ 32%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/gzwrite.c.o\n",
      "[ 32%] Built target format_reader\n",
      "[ 33%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/inflate.c.o\n",
      "[ 35%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/infback.c.o\n",
      "[ 37%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/inftrees.c.o\n",
      "[ 39%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/inffast.c.o\n",
      "[ 41%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/trees.c.o\n",
      "[ 43%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/uncompr.c.o\n",
      "[ 45%] Building C object thirdparty/zlib/CMakeFiles/zlib.dir/zlib/zutil.c.o\n",
      "[ 47%] Linking C static library ../../intel64/Release/lib/libzlib.a\n",
      "[ 47%] Built target zlib\n",
      "Scanning dependencies of target cnpy\n",
      "[ 49%] Building CXX object thirdparty/cnpy/CMakeFiles/cnpy.dir/cnpy.cpp.o\n",
      "[ 50%] Linking CXX static library ../../../intel64/Release/lib/libgflags_nothreads.a\n",
      "[ 50%] Built target gflags_nothreads_static\n",
      "Scanning dependencies of target ie_samples_utils\n",
      "[ 52%] Building CXX object common/utils/CMakeFiles/ie_samples_utils.dir/src/args_helper.cpp.o\n",
      "[ 54%] Building CXX object common/utils/CMakeFiles/ie_samples_utils.dir/src/common.cpp.o\n",
      "[ 56%] Building CXX object common/utils/CMakeFiles/ie_samples_utils.dir/src/slog.cpp.o\n",
      "[ 58%] Linking CXX static library ../../intel64/Release/lib/libcnpy.a\n",
      "[ 58%] Built target cnpy\n",
      "[ 60%] Linking CXX static library ../../intel64/Release/lib/libie_samples_utils.a\n",
      "[ 60%] Built target ie_samples_utils\n",
      "Scanning dependencies of target hello_query_device\n",
      "Scanning dependencies of target classification_sample_async\n",
      "Scanning dependencies of target hello_classification\n",
      "Scanning dependencies of target hello_nv12_input_classification\n",
      "Scanning dependencies of target speech_sample\n",
      "Scanning dependencies of target hello_reshape_ssd\n",
      "Scanning dependencies of target model_creation_sample\n",
      "Scanning dependencies of target benchmark_app\n",
      "[ 62%] Building CXX object classification_sample_async/CMakeFiles/classification_sample_async.dir/main.cpp.o\n",
      "[ 64%] Building CXX object hello_nv12_input_classification/CMakeFiles/hello_nv12_input_classification.dir/main.cpp.o\n",
      "[ 66%] Building CXX object hello_classification/CMakeFiles/hello_classification.dir/main.cpp.o\n",
      "[ 67%] Building CXX object hello_query_device/CMakeFiles/hello_query_device.dir/main.cpp.o\n",
      "[ 69%] Building CXX object speech_sample/CMakeFiles/speech_sample.dir/fileutils.cpp.o\n",
      "[ 71%] Building CXX object hello_reshape_ssd/CMakeFiles/hello_reshape_ssd.dir/main.cpp.o\n",
      "[ 73%] Building CXX object model_creation_sample/CMakeFiles/model_creation_sample.dir/main.cpp.o\n",
      "[ 75%] Building CXX object benchmark_app/CMakeFiles/benchmark_app.dir/inputs_filling.cpp.o\n",
      "[ 77%] Linking CXX executable ../intel64/Release/hello_query_device\n",
      "[ 79%] Building CXX object speech_sample/CMakeFiles/speech_sample.dir/main.cpp.o\n",
      "[ 79%] Built target hello_query_device\n",
      "[ 81%] Building CXX object benchmark_app/CMakeFiles/benchmark_app.dir/main.cpp.o\n",
      "[ 83%] Linking CXX executable ../intel64/Release/hello_classification\n",
      "[ 84%] Linking CXX executable ../intel64/Release/hello_reshape_ssd\n",
      "[ 84%] Built target hello_classification\n",
      "[ 86%] Linking CXX executable ../intel64/Release/hello_nv12_input_classification\n",
      "[ 88%] Building CXX object benchmark_app/CMakeFiles/benchmark_app.dir/remote_tensors_filling.cpp.o\n",
      "[ 88%] Built target hello_reshape_ssd\n",
      "[ 90%] Building CXX object benchmark_app/CMakeFiles/benchmark_app.dir/statistics_report.cpp.o\n",
      "[ 90%] Built target hello_nv12_input_classification\n",
      "[ 92%] Linking CXX executable ../intel64/Release/classification_sample_async\n",
      "[ 94%] Building CXX object benchmark_app/CMakeFiles/benchmark_app.dir/utils.cpp.o\n",
      "[ 94%] Built target classification_sample_async\n",
      "[ 96%] Linking CXX executable ../intel64/Release/model_creation_sample\n",
      "[ 96%] Built target model_creation_sample\n",
      "[ 98%] Linking CXX executable ../intel64/Release/speech_sample\n",
      "[ 98%] Built target speech_sample\n",
      "[100%] Linking CXX executable ../intel64/Release/benchmark_app\n",
      "[100%] Built target benchmark_app\n",
      "Scanning dependencies of target ie_samples\n",
      "[100%] Built target ie_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Build completed, you can find binaries for all samples in the /home/openvino/inference_engine_cpp_samples_build/intel64/Release subfolder.\n",
      "\n",
      "[ INFO ] OpenVINO Runtime version ......... 2022.2.0\n",
      "[ INFO ] Build ........... 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ INFO ] \n",
      "[E:] [BSL] found 0 ioexpander device\n",
      "\u001b[35mE: [xLinkUsb] [    599966] [hello_query_dev] usb_find_device_with_bcd:266\tLibrary has not been initialized when loaded\u001b[0m\n",
      "[ INFO ] Available devices: \n",
      "[ INFO ] CPU\n",
      "[ INFO ] \tSUPPORTED_PROPERTIES: \n",
      "[ INFO ] \t\tImmutable: AVAILABLE_DEVICES : \"\"\n",
      "[ INFO ] \t\tImmutable: RANGE_FOR_ASYNC_INFER_REQUESTS : 1 1 1\n",
      "[ INFO ] \t\tImmutable: RANGE_FOR_STREAMS : 1 96\n",
      "[ INFO ] \t\tImmutable: FULL_DEVICE_NAME : Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "[ INFO ] \t\tImmutable: OPTIMIZATION_CAPABILITIES : WINOGRAD FP32 FP16 INT8 BIN EXPORT_IMPORT\n",
      "[ INFO ] \t\tImmutable: CACHE_DIR : \"\"\n",
      "[ INFO ] \t\tMutable: NUM_STREAMS : 1\n",
      "[ INFO ] \t\tMutable: AFFINITY : CORE\n",
      "[ INFO ] \t\tMutable: INFERENCE_NUM_THREADS : 0\n",
      "[ INFO ] \t\tMutable: PERF_COUNT : NO\n",
      "[ INFO ] \t\tMutable: INFERENCE_PRECISION_HINT : f32\n",
      "[ INFO ] \t\tMutable: PERFORMANCE_HINT : \"\"\n",
      "[ INFO ] \t\tMutable: PERFORMANCE_HINT_NUM_REQUESTS : 0\n",
      "[ INFO ] \n",
      "[ INFO ] GNA\n",
      "[ INFO ] \tSUPPORTED_PROPERTIES: \n",
      "[ INFO ] \t\tImmutable: AVAILABLE_DEVICES : GNA_SW\n",
      "[ INFO ] \t\tImmutable: OPTIMAL_NUMBER_OF_INFER_REQUESTS : 1\n",
      "[ INFO ] \t\tImmutable: RANGE_FOR_ASYNC_INFER_REQUESTS : 1 1 1\n",
      "[ INFO ] \t\tImmutable: OPTIMIZATION_CAPABILITIES : INT16 INT8 EXPORT_IMPORT\n",
      "[ INFO ] \t\tImmutable: FULL_DEVICE_NAME : GNA_SW\n",
      "[ INFO ] \t\tImmutable: GNA_LIBRARY_FULL_VERSION : 3.0.0.1455\n",
      "[ INFO ] \t\tMutable: GNA_SCALE_FACTOR_PER_INPUT : \"\"\n",
      "[ INFO ] \t\tMutable: GNA_FIRMWARE_MODEL_IMAGE : \"\"\n",
      "[ INFO ] \t\tMutable: GNA_DEVICE_MODE : GNA_SW_EXACT\n",
      "[ INFO ] \t\tMutable: GNA_HW_EXECUTION_TARGET : UNDEFINED\n",
      "[ INFO ] \t\tMutable: GNA_HW_COMPILE_TARGET : UNDEFINED\n",
      "[ INFO ] \t\tMutable: GNA_PWL_DESIGN_ALGORITHM : UNDEFINED\n",
      "[ INFO ] \t\tMutable: GNA_PWL_MAX_ERROR_PERCENT : 1.000000\n",
      "[ INFO ] \t\tMutable: PERFORMANCE_HINT : \"\"\n",
      "[ INFO ] \t\tMutable: INFERENCE_PRECISION_HINT : undefined\n",
      "[ INFO ] \t\tMutable: PERFORMANCE_HINT_NUM_REQUESTS : 1\n",
      "[ INFO ] \t\tMutable: LOG_LEVEL : LOG_NONE\n",
      "[ INFO ] \n"
     ]
    }
   ],
   "source": [
    "!docker run openvino/ubuntu20_runtime:2022.2.0 bash -c \"./samples/cpp/build_samples.sh && \\\n",
    "/home/openvino/inference_engine_cpp_samples_build/intel64/Release/hello_query_device\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Using AI accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rChqvcLYoORI"
   },
   "source": [
    "Of course, you can run a container and inference on different Intel devices like iGPU (Intel® Processor Graphics), NCS2(Intel® Neural Compute Stick 2) or HDDL (Intel® Vision Accelerator Design with Intel® Movidius™ VPUs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQmiZRAVp3T0"
   },
   "source": [
    "To enable GPU access, add `--device /dev/dri` to `docker run`. You might also need to add to the container security context a group with permission to the GPU character device.\n",
    "\n",
    "You can find more information about how to run a container with access to a specific device on our [Docker CI GitHub repo](https://github.com/openvinotoolkit/docker_ci/blob/master/get-started.md#run-a-container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] OpenVINO Runtime version ......... 2022.2.0\n",
      "[ INFO ] Build ........... 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ INFO ]\n",
      "[E:] [BSL] found 0 ioexpander device\n",
      "E: [xLinkUsb] [    112159] [hello_query_dev] usb_find_device_with_bcd:266       Library has not been initialized when loaded\n",
      "[ INFO ] Available devices:\n",
      "[ INFO ] CPU\n",
      "[ INFO ]        SUPPORTED_PROPERTIES:\n",
      "[ INFO ]                Immutable: AVAILABLE_DEVICES : \"\"\n",
      "[ INFO ]                Immutable: RANGE_FOR_ASYNC_INFER_REQUESTS : 1 1 1\n",
      "[ INFO ]                Immutable: RANGE_FOR_STREAMS : 1 8\n",
      "[ INFO ]                Immutable: FULL_DEVICE_NAME : Intel(R) Core(TM) i3-10100 CPU @ 3.60GHz\n",
      "[ INFO ]                Immutable: OPTIMIZATION_CAPABILITIES : FP32 FP16 INT8 BIN EXPORT_IMPORT\n",
      "[ INFO ]                Immutable: CACHE_DIR : \"\"\n",
      "[ INFO ]                Mutable: NUM_STREAMS : 1\n",
      "[ INFO ]                Mutable: AFFINITY : CORE\n",
      "[ INFO ]                Mutable: INFERENCE_NUM_THREADS : 0\n",
      "[ INFO ]                Mutable: PERF_COUNT : NO\n",
      "[ INFO ]                Mutable: INFERENCE_PRECISION_HINT : f32\n",
      "[ INFO ]                Mutable: PERFORMANCE_HINT : \"\"\n",
      "[ INFO ]                Mutable: PERFORMANCE_HINT_NUM_REQUESTS : 0\n",
      "[ INFO ]\n",
      "[ INFO ] GNA\n",
      "[ INFO ]        SUPPORTED_PROPERTIES:\n",
      "[ INFO ]                Immutable: AVAILABLE_DEVICES : GNA_SW\n",
      "[ INFO ]                Immutable: OPTIMAL_NUMBER_OF_INFER_REQUESTS : 1\n",
      "[ INFO ]                Immutable: RANGE_FOR_ASYNC_INFER_REQUESTS : 1 1 1\n",
      "[ INFO ]                Immutable: OPTIMIZATION_CAPABILITIES : INT16 INT8 EXPORT_IMPORT\n",
      "[ INFO ]                Immutable: FULL_DEVICE_NAME : GNA_SW\n",
      "[ INFO ]                Immutable: GNA_LIBRARY_FULL_VERSION : 3.0.0.1455\n",
      "[ INFO ]                Mutable: GNA_SCALE_FACTOR_PER_INPUT : \"\"\n",
      "[ INFO ]                Mutable: GNA_FIRMWARE_MODEL_IMAGE : \"\"\n",
      "[ INFO ]                Mutable: GNA_DEVICE_MODE : GNA_SW_EXACT\n",
      "[ INFO ]                Mutable: GNA_HW_EXECUTION_TARGET : UNDEFINED\n",
      "[ INFO ]                Mutable: GNA_HW_COMPILE_TARGET : UNDEFINED\n",
      "[ INFO ]                Mutable: GNA_PWL_DESIGN_ALGORITHM : UNDEFINED\n",
      "[ INFO ]                Mutable: GNA_PWL_MAX_ERROR_PERCENT : 1.000000\n",
      "[ INFO ]                Mutable: PERFORMANCE_HINT : \"\"\n",
      "[ INFO ]                Mutable: INFERENCE_PRECISION_HINT : undefined\n",
      "[ INFO ]                Mutable: PERFORMANCE_HINT_NUM_REQUESTS : 1\n",
      "[ INFO ]                Mutable: LOG_LEVEL : LOG_NONE\n",
      "[ INFO ]\n",
      "[ INFO ] GPU\n",
      "[ INFO ]        SUPPORTED_PROPERTIES:\n",
      "[ INFO ]                Immutable: AVAILABLE_DEVICES : 0\n",
      "[ INFO ]                Immutable: RANGE_FOR_ASYNC_INFER_REQUESTS : 1 2 1\n",
      "[ INFO ]                Immutable: RANGE_FOR_STREAMS : 1 2\n",
      "[ INFO ]                Immutable: OPTIMAL_BATCH_SIZE : 1\n",
      "[ INFO ]                Immutable: MAX_BATCH_SIZE : 1\n",
      "[ INFO ]                Immutable: FULL_DEVICE_NAME : Intel(R) UHD Graphics 630 [0x9bc8] (iGPU)\n",
      "[ INFO ]                Immutable: DEVICE_UUID : 00000000000000000000000000000000\n",
      "[ INFO ]                Immutable: DEVICE_TYPE : integrated\n",
      "[ INFO ]                Immutable: DEVICE_GOPS : f16 809.6 f32 404.8 i8 404.8 u8 404.8\n",
      "[ INFO ]                Immutable: OPTIMIZATION_CAPABILITIES : FP32 BIN FP16\n",
      "[ INFO ]                Immutable: GPU_DEVICE_TOTAL_MEM_SIZE : 53773582336\n",
      "[ INFO ]                Immutable: GPU_UARCH_VERSION : 9.0.0\n",
      "[ INFO ]                Immutable: GPU_EXECUTION_UNITS_COUNT : 23\n",
      "[ INFO ]                Immutable: GPU_MEMORY_STATISTICS : \"\"\n",
      "[ INFO ]                Mutable: PERF_COUNT : NO\n",
      "[ INFO ]                Mutable: MODEL_PRIORITY : MEDIUM\n",
      "[ INFO ]                Mutable: GPU_HOST_TASK_PRIORITY : MEDIUM\n",
      "[ INFO ]                Mutable: GPU_QUEUE_PRIORITY : MEDIUM\n",
      "[ INFO ]                Mutable: GPU_QUEUE_THROTTLE : MEDIUM\n",
      "[ INFO ]                Mutable: GPU_ENABLE_LOOP_UNROLLING : YES\n",
      "[ INFO ]                Mutable: CACHE_DIR : \"\"\n",
      "[ INFO ]                Mutable: PERFORMANCE_HINT : \"\"\n",
      "[ INFO ]                Mutable: COMPILATION_NUM_THREADS : 8\n",
      "[ INFO ]                Mutable: NUM_STREAMS : 1\n",
      "[ INFO ]                Mutable: PERFORMANCE_HINT_NUM_REQUESTS : 0\n",
      "[ INFO ]                Mutable: DEVICE_ID : 0\n",
     "[ INFO ]\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --device /dev/dri --group-add=$(stat -c \"%g\" /dev/dri/render* ) openvino/ubuntu20_dev:2022.2.0 ./samples/cpp/samples_bin/hello_query_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about how to run a container with access to a specific device on our [Docker CI GitHub repo](https://github.com/openvinotoolkit/docker_ci/blob/master/get-started.md#run-a-container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTRsBdVl0gUt"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this article, we briefly introduced the speech recognition demo using OpenVINO Docker images. Of course, there is much more to try. We hope that this article has motivated you to try it yourself and maybe continue to explore all the possibilities of OpenVINO Docker images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYGLL16p0AsU"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "\n",
    "*   [Available OpenVINO Docker images](https://github.com/openvinotoolkit/docker_ci#prebuilt-images)\n",
    "*   [Docker CI framework for Intel® Distribution of OpenVINO™ toolkit](https://github.com/openvinotoolkit/docker_ci). The Framework can generate a Dockerfile, build, test, and deploy an image with the Intel® Distribution of OpenVINO™ toolkit. You can reuse available Dockerfiles, add your layer and customize the image of OpenVINO™ for your needs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OV-docker-usage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5cc1b652180353be18e737b02ed7addecb006324dc9a4b32468a11c9aff8e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
